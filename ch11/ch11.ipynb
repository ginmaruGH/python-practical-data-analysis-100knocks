{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共通で利用するライブラリ\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams['font.family'] = 'IPAexGothic'\n",
    "\n",
    "# この章で利用するライブラリ\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_imshow(img):\n",
    "    \"\"\"画像をNotebook上のインラインに表示する\"\"\"\n",
    "    img = cv2.imencode(\".png\", img)[1]\n",
    "    display(Image(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11章　深層学習に挑戦する10本ノック\n",
    "ここでは、深層学習を学ぶうえで必要なプログラムを実行していく流れを学ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前提条件\n",
    "\n",
    "- 小売店の前を通る道路の画像・映像\n",
    "  - img01.jpg（道路の通行人の画像（道路全体））\n",
    "  - img02.jpg（自動車の画像）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knock101: 深層学習に必要なデータを準備する\n",
    "\n",
    "- MNITS\n",
    "  - 手書き数字画像データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "\n",
    "mnist = datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 形状の出力\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0番目のデータの形状の出力\n",
    "\n",
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABLklEQVQoFX3BTyhDARzA8e8vciC2drFS0hymFpFQEsm/iLK4UC6Uy0ipXZSDw7TadkAOO60c3NmJ/JmLWpZ/F7n7c7TSa8ger7337M3hfT6CDcGGYEOwIdgQLMocsFTpDURnPsIbgGCor+jucU6hebryK3frKUDQtZ060KnzCi9vj2gEnSvtQZPO9n85MAmGyfGbbW57Fd/KIibBVPMeX5jbx0ooiqxeDKpYCEVVyb7RYywEi8br7Hlm9weTYOVPVLO294pBKNEcGyAeekYnlHJOJORsCJ3w32f590iKAqFEy3THMPftKgWChXfZ74b8yRg64Y97NtAAZEKHGARDrW+nCUhHDlRMQoEr3uoBLmNHOYoETVewsw7IbW0qWAmacBAekvlollKCDcGGYOMXIl1LHQ8zJJMAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 0番目のデータの表示\n",
    "\n",
    "f_imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0番目のデータの正解データ\n",
    "\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深層学習を行うためのデータの変換\n",
    "\n",
    "# 正則化（各データを最大値で割る。0~1の値に変換する）\n",
    "X_train_sc, X_test_sc = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形状を整える\n",
    "\n",
    "# 28*28pixelの次元に、チャンネル数の次元を加える\n",
    "# 28 * 28 * チャンネル数（モノクロなので「1」）\n",
    "X_train_sc = X_train_sc.reshape((60_000, 28, 28, 1))\n",
    "X_tets_sc = X_test_sc.reshape((10_000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knock102: 深層学習モデルを構築する\n",
    "\n",
    "- 多層ニューラルネットワーク\n",
    "- CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 11:42:36.914034: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# ニューラルネットワークモデル定義\n",
    "\n",
    "model1 = models.Sequential()\n",
    "model1.add(layers.Flatten(input_shape=(28, 28)))\n",
    "model1.add(layers.Dense(512, activation=\"relu\"))\n",
    "model1.add(layers.Dropout(0.2))\n",
    "model1.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNモデル定義\n",
    "\n",
    "model2 = models.Sequential()\n",
    "model2.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "model2.add(layers.MaxPooling2D(2, 2))\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model2.add(layers.MaxPooling2D(2, 2))\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(64, activation=\"relu\"))\n",
    "model2.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 576)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                36928     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2181 - accuracy: 0.9344\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0950 - accuracy: 0.9714\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0668 - accuracy: 0.9791\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0524 - accuracy: 0.9828\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0435 - accuracy: 0.9858\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0353 - accuracy: 0.9885\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0318 - accuracy: 0.9890\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0278 - accuracy: 0.9908\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0247 - accuracy: 0.9916\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0241 - accuracy: 0.9918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1429376a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ニューラルネットワークモデル構築\n",
    "\n",
    "model1.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model1.fit(X_train_sc, y_train, epochs=10)\n",
    "# 1m45s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.1489 - accuracy: 0.9539\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.0461 - accuracy: 0.9858\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 40s 22ms/step - loss: 0.0315 - accuracy: 0.9900\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.0266 - accuracy: 0.9917\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.0202 - accuracy: 0.9937\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.0161 - accuracy: 0.9951\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.0133 - accuracy: 0.9957\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 42s 23ms/step - loss: 0.0113 - accuracy: 0.9964\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 0.0100 - accuracy: 0.9969\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.0086 - accuracy: 0.9971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14333ee20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNNモデル構築\n",
    "\n",
    "model2.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model2.fit(X_train_sc, y_train, epochs=10)\n",
    "# 8m20s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knock103: モデルを評価する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0845 - accuracy: 0.9799\n",
      "0.9799000024795532\n"
     ]
    }
   ],
   "source": [
    "# model1の正解率\n",
    "\n",
    "model1_test_los, model1_test_acc = model1.evaluate(X_test_sc, y_test)\n",
    "print(model1_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0366 - accuracy: 0.9921\n",
      "0.9921000003814697\n"
     ]
    }
   ],
   "source": [
    "# model2の正解率\n",
    "\n",
    "model2_test_los, model2_test_acc = model2.evaluate(X_test_sc, y_test)\n",
    "print(model2_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knock104: モデルを使った予測をする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 9s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# 予測の実行\n",
    "\n",
    "predictions = model2.predict(X_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 予測結果の形状\n",
    "# 「60000」の画像と「10」クラスそれぞれに対する類似度合いを計算した結果\n",
    "\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0番目の出力\n",
    "\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# もっとも高い確率の予測数字\n",
    "\n",
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正解データ\n",
    "\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knock105: 物体検出YOLOを使って人の検出を行う\n",
    "\n",
    "- 物体家出を行う深層学習ネットワーク\n",
    "- YOLOv3\n",
    "- YOLOv3-tiny\n",
    "- [yolov3-tf2](https://github.com/zzh8829/yolov3-tf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOv3-tinyの準備\n",
    "\n",
    "```bash\n",
    "# yolov3-tf2のダウンロード\n",
    "!git clone https://github.com/zzh8829/yolov3-tf2.git./yolov3_tf2\n",
    "%cd ./yolov3_tf2\n",
    "!git checkout c43df87d8582699aea8e9768b4ebe8d7fe1c6b4c\n",
    "%cd ../\n",
    "```\n",
    "\n",
    "```bash\n",
    "#YOLOの学習済みモデルのダウンロード\n",
    "!wget https://pjreddie.com/media/files/yolov3-tiny.weights \n",
    "```\n",
    "\n",
    "```bash\n",
    "#ダウンロードしたYOLOの学習済みモデルをKerasから利用出来る形に変換\n",
    "!python3 yolov3_tf2/convert.py --weights yolov3-tiny.weights --output  yolov3_tf2/checkpoints/yolov3-tiny.tf --tiny\n",
    "\n",
    "\n",
    "2022-08-26 09:02:36.505321: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
    "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
    "Model: \"yolov3_tiny\"\n",
    "__________________________________________________________________________________________________\n",
    " Layer (type)                   Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    " input (InputLayer)             [(None, None, None,  0           []                               \n",
    "                                 3)]                                                              \n",
    "                                                                                                  \n",
    " yolo_darknet (Functional)      ((None, None, None,  6298480     ['input[0][0]']                  \n",
    "                                 256),                                                            \n",
    "                                 (None, None, None,                                               \n",
    "                                 1024))                                                           \n",
    "                                                                                                  \n",
    " yolo_conv_0 (Functional)       (None, None, None,   263168      ['yolo_darknet[0][1]']           \n",
    "                                256)                                                              \n",
    "                                                                                                  \n",
    " yolo_conv_1 (Functional)       (None, None, None,   33280       ['yolo_conv_0[0][0]',            \n",
    "                                384)                              'yolo_darknet[0][0]']           \n",
    "                                                                                                  \n",
    " yolo_output_0 (Functional)     (None, None, None,   1312511     ['yolo_conv_0[0][0]']            \n",
    "                                3, 85)                                                            \n",
    "                                                                                                  \n",
    " yolo_output_1 (Functional)     (None, None, None,   951295      ['yolo_conv_1[0][0]']            \n",
    "                                3, 85)                                                            \n",
    "                                                                                                  \n",
    " yolo_boxes_0 (Lambda)          ((None, None, None,  0           ['yolo_output_0[0][0]']          \n",
    "                                 3, 4),                                                           \n",
    "                                 (None, None, None,                                               \n",
    "                                 3, 1),                                                           \n",
    "                                 (None, None, None,                                               \n",
    "                                 3, 80),                                                          \n",
    "                                 (None, None, None,                                               \n",
    "                                 3, 4))                                                           \n",
    "                                                                                                  \n",
    " yolo_boxes_1 (Lambda)          ((None, None, None,  0           ['yolo_output_1[0][0]']          \n",
    "                                 3, 4),                                                           \n",
    "                                 (None, None, None,                                               \n",
    "                                 3, 1),                                                           \n",
    "                                 (None, None, None,                                               \n",
    "                                 3, 80),                                                          \n",
    "                                 (None, None, None,                                               \n",
    "                                 3, 4))                                                           \n",
    "                                                                                                  \n",
    " yolo_nms (Lambda)              ((1, None, 4),       0           ['yolo_boxes_0[0][0]',           \n",
    "                                 (1, None),                       'yolo_boxes_0[0][1]',           \n",
    "                                 (1, None),                       'yolo_boxes_0[0][2]',           \n",
    "                                 (1,))                            'yolo_boxes_1[0][0]',           \n",
    "                                                                  'yolo_boxes_1[0][1]',           \n",
    "                                                                  'yolo_boxes_1[0][2]']           \n",
    "                                                                                                  \n",
    "==================================================================================================\n",
    "Total params: 8,858,734\n",
    "Trainable params: 8,852,366\n",
    "Non-trainable params: 6,368\n",
    "__________________________________________________________________________________________________\n",
    "I0826 09:02:37.032343 4505515520 convert.py:24] model created\n",
    "I0826 09:02:37.033011 4505515520 utils.py:44] yolo_darknet/conv2d bn\n",
    "I0826 09:02:37.034296 4505515520 utils.py:44] yolo_darknet/conv2d_1 bn\n",
    "I0826 09:02:37.035688 4505515520 utils.py:44] yolo_darknet/conv2d_2 bn\n",
    "I0826 09:02:37.037266 4505515520 utils.py:44] yolo_darknet/conv2d_3 bn\n",
    "I0826 09:02:37.039098 4505515520 utils.py:44] yolo_darknet/conv2d_4 bn\n",
    "I0826 09:02:37.043732 4505515520 utils.py:44] yolo_darknet/conv2d_5 bn\n",
    "I0826 09:02:37.049914 4505515520 utils.py:44] yolo_darknet/conv2d_6 bn\n",
    "I0826 09:02:37.088458 4505515520 utils.py:44] yolo_conv_0/conv2d_7 bn\n",
    "I0826 09:02:37.092405 4505515520 utils.py:44] yolo_output_0/conv2d_8 bn\n",
    "I0826 09:02:37.097918 4505515520 utils.py:44] yolo_output_0/conv2d_9 bias\n",
    "I0826 09:02:37.099339 4505515520 utils.py:44] yolo_conv_1/conv2d_10 bn\n",
    "I0826 09:02:37.100522 4505515520 utils.py:44] yolo_output_1/conv2d_11 bn\n",
    "I0826 09:02:37.104185 4505515520 utils.py:44] yolo_output_1/conv2d_12 bias\n",
    "I0826 09:02:37.105022 4505515520 convert.py:27] weights loaded\n",
    "I0826 09:02:37.261889 4505515520 convert.py:31] sanity check passed\n",
    "I0826 09:02:37.432790 4505515520 convert.py:34] weights saved\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOv3-tinyによる物体検出の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yolov3']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from absl import app, logging, flags\n",
    "from absl.flags import FLAGS\n",
    "\n",
    "app._run_init([\"yolov3\"], app.parse_flags_with_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x16c05d340>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習済みの重みをそのまま利用する\n",
    "\n",
    "import tensorflow as tf\n",
    "from yolov3_tf2.models import YoloV3Tiny, YoloLoss\n",
    "from yolov3_tf2.dataset import transform_images\n",
    "from yolov3_tf2.utils import draw_outputs\n",
    "\n",
    "FLAGS.yolo_iou_threshold = 0.5\n",
    "FLAGS.yolo_score_threshold = 0.5\n",
    "\n",
    "yolo_class_names = [c.strip() for c in open(\"./yolov3_tf2/data/coco.names\").readlines()]\n",
    "\n",
    "yolo = YoloV3Tiny(classes=80)\n",
    "\n",
    "# 重みの読み込み\n",
    "yolo.load_weights(\"./yolov3_tf2/checkpoints/yolov3-tiny.tf\").expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 887ms/step\n"
     ]
    }
   ],
   "source": [
    "img_filename = \"../support/11章/img/img01.jpg\"\n",
    "img_raw_p = tf.image.decode_and_crop_jpeg(\n",
    "    contents=open(img_filename, \"rb\").read(),\n",
    "    crop_window=[0, 0, 1440, 1920],\n",
    "    channels=3\n",
    ")\n",
    "data_shape = (256, 256, 3)\n",
    "img_yolo_p = transform_images(img_raw_p, data_shape[0])\n",
    "img_yolo_p = np.expand_dims(img_yolo_p, 0)\n",
    "\n",
    "# 予測開始\n",
    "boxes, scores, classes, nums = yolo.predict(img_yolo_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 結果の出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "cannot open resource",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mセル37 を /Users/takeru/Library/CloudStorage/OneDrive-個人用/Learn/Python/python-practical-data-analysis-100knocks/ch11/ch11.ipynb\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/takeru/Library/CloudStorage/OneDrive-%E5%80%8B%E4%BA%BA%E7%94%A8/Learn/Python/python-practical-data-analysis-100knocks/ch11/ch11.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m img_yolo_p \u001b[39m=\u001b[39m img_raw_p\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/takeru/Library/CloudStorage/OneDrive-%E5%80%8B%E4%BA%BA%E7%94%A8/Learn/Python/python-practical-data-analysis-100knocks/ch11/ch11.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m img_yolo_p \u001b[39m=\u001b[39m draw_outputs(img_yolo_p, (boxes, scores, classes, nums), yolo_class_names)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/takeru/Library/CloudStorage/OneDrive-%E5%80%8B%E4%BA%BA%E7%94%A8/Learn/Python/python-practical-data-analysis-100knocks/ch11/ch11.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/takeru/Library/CloudStorage/OneDrive-%E5%80%8B%E4%BA%BA%E7%94%A8/Learn/Python/python-practical-data-analysis-100knocks/ch11/ch11.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(img_yolo_p)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-個人用/Learn/Python/python-practical-data-analysis-100knocks/venv/lib/python3.9/site-packages/yolov3_tf2/utils.py:112\u001b[0m, in \u001b[0;36mdraw_outputs\u001b[0;34m(img, outputs, class_names)\u001b[0m\n\u001b[1;32m    110\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[1;32m    111\u001b[0m draw \u001b[39m=\u001b[39m ImageDraw\u001b[39m.\u001b[39mDraw(img)\n\u001b[0;32m--> 112\u001b[0m font \u001b[39m=\u001b[39m ImageFont\u001b[39m.\u001b[39;49mtruetype(font\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data/fonts/futur.ttf\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    113\u001b[0m                           size\u001b[39m=\u001b[39;49m(img\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m] \u001b[39m+\u001b[39;49m img\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m]) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m100\u001b[39;49m)\n\u001b[1;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nums):\n\u001b[1;32m    115\u001b[0m     color \u001b[39m=\u001b[39m colors[\u001b[39mint\u001b[39m(classes[i])]\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-個人用/Learn/Python/python-practical-data-analysis-100knocks/venv/lib/python3.9/site-packages/PIL/ImageFont.py:959\u001b[0m, in \u001b[0;36mtruetype\u001b[0;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[39mreturn\u001b[39;00m FreeTypeFont(font, size, index, encoding, layout_engine)\n\u001b[1;32m    958\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 959\u001b[0m     \u001b[39mreturn\u001b[39;00m freetype(font)\n\u001b[1;32m    960\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_path(font):\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-個人用/Learn/Python/python-practical-data-analysis-100knocks/venv/lib/python3.9/site-packages/PIL/ImageFont.py:956\u001b[0m, in \u001b[0;36mtruetype.<locals>.freetype\u001b[0;34m(font)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfreetype\u001b[39m(font):\n\u001b[0;32m--> 956\u001b[0m     \u001b[39mreturn\u001b[39;00m FreeTypeFont(font, size, index, encoding, layout_engine)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-個人用/Learn/Python/python-practical-data-analysis-100knocks/venv/lib/python3.9/site-packages/PIL/ImageFont.py:247\u001b[0m, in \u001b[0;36mFreeTypeFont.__init__\u001b[0;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 load_from_bytes(f)\n\u001b[1;32m    246\u001b[0m             \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfont \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49mgetfont(\n\u001b[1;32m    248\u001b[0m         font, size, index, encoding, layout_engine\u001b[39m=\u001b[39;49mlayout_engine\n\u001b[1;32m    249\u001b[0m     )\n\u001b[1;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     load_from_bytes(font)\n",
      "\u001b[0;31mOSError\u001b[0m: cannot open resource"
     ]
    }
   ],
   "source": [
    "img_yolo_p = img_raw_p.numpy()\n",
    "img_yolo_p = draw_outputs(img_yolo_p, (boxes, scores, classes, nums), yolo_class_names)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img_yolo_p)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knock106: YOLOの学習を行うための準備をする"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81dbdffa2555d8332052cc5706d1001114a92c16400f745eaf97970677d8084c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
